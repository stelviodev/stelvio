# AWS S3 (Object Storage)

## S3 Buckets

Stelvio supports creating and managing S3 buckets using the `Bucket` component.

Create an S3 bucket and link it to an API Gateway handler:

```python
@app.run
def run() -> None:
    bucket = Bucket("todo-bucket")

    api = Api("todo-api")
    api.route("GET", "/write", handler="functions/write.get", links=[bucket])
    api.route("GET", "/read", handler="functions/read.get", links=[bucket])
```

Using the [linking mechanism](/guides/linking), you can easily access the S3 bucket in your Lambda functions using the regular [`boto3`](https://boto3.amazonaws.com/) library:

```python
import boto3
from stlv_resources import Resources


def get(event, context):
    s3_client = boto3.client("s3")
    bucket_name = Resources.todo_bucket.bucket_name
    s3_client.put_object(Bucket=bucket_name, Key="hello.txt", Body="Hello, World!")
    return {"statusCode": 200, "body": "Hello, World!"}
```

### Public Access

By default, all public access is blocked for S3 buckets created with the `Bucket` component.

You can change that behaviour by setting the `access` argument to `'public'`:

```python
@app.run
def run() -> None:
    bucket = Bucket("todo-bucket", access="public")
```

??? note "How access is implemented"

    Internally, access to S3 buckets is handled by the `BucketPublicAccessBlock` resource.

    It is created with the followign parameters:

    ```python
        block_public_acls=<Value>,
        block_public_policy=<Value>,
        ignore_public_acls=<Value>,
        restrict_public_buckets=<Value>,
    ```

    `<Value>` is set to either `False` for public access or `True` for private access (default).

    | Parameter                | Description                                                                 |
    |--------------------------|-----------------------------------------------------------------------------|
    | `block_public_acls`      | Whether to block public ACLs.                                               |
    | `block_public_policy`    | Whether to block public policies.                                           |
    | `ignore_public_acls`     | Whether to ignore public ACLs.                                              |
    | `restrict_public_buckets`| Whether to restrict public buckets.                                         |

    Additionally, a policy for `s3:GetObject` is created to allow public read access to the objects in the bucket if `access` is set to `'public'`.

    See the [Pulumi Documentation](https://www.pulumi.com/registry/packages/aws/api-docs/s3/bucketpublicaccessblock/) for more information.

### Event Notifications

You can subscribe to S3 bucket events using the `notify` method. Event notifications can trigger either Lambda functions or SQS queues when objects are created, deleted, or modified.

#### Function Notifications

Subscribe a Lambda function to handle S3 events:

```python
@app.run
def run() -> None:
    bucket = Bucket("uploads")

    # Notify on all object created events
    bucket.notify(
        "on-upload",
        events=["s3:ObjectCreated:*"],
        function="functions/process_upload.handler",
    )
```

You can also filter notifications by object key prefix or suffix:

```python
@app.run
def run() -> None:
    bucket = Bucket("media")

    # Only trigger for images in the uploads folder
    bucket.notify(
        "process-images",
        events=["s3:ObjectCreated:*"],
        filter_prefix="uploads/",
        filter_suffix=".jpg",
        function="functions/process_image.handler",
        memory=512,
        timeout=60,
    )
```

#### Linking Resources

Grant the notification function access to other resources using `links`:

```python
@app.run
def run() -> None:
    bucket = Bucket("uploads")
    table = DynamoTable("metadata", fields={"pk": "string"}, partition_key="pk")
    results_queue = Queue("processing-results")

    # Notification function can access the table and queue
    bucket.notify(
        "on-upload",
        events=["s3:ObjectCreated:*"],
        function="functions/process_upload.handler",
        links=[table, results_queue],
    )
```

#### Queue Notifications

Send event notifications to an SQS queue for asynchronous processing:

```python
@app.run
def run() -> None:
    bucket = Bucket("orders")
    processing_queue = Queue("order-processing")

    # Send notifications to queue
    bucket.notify(
        "order-created",
        events=["s3:ObjectCreated:Put"],
        queue=processing_queue,
    )

    # Subscribe a function to process from the queue
    processing_queue.subscribe("processor", "functions/process_order.handler")
```

#### Multiple Notifications

You can add multiple notifications to the same bucket:

```python
@app.run
def run() -> None:
    bucket = Bucket("documents")

    # Different handlers for different events
    bucket.notify(
        "on-create",
        events=["s3:ObjectCreated:*"],
        function="functions/handle_create.handler",
    )

    bucket.notify(
        "on-delete",
        events=["s3:ObjectRemoved:*"],
        function="functions/handle_delete.handler",
    )
```

!!! warning "Notifications must be defined before resource creation"
    All notifications must be added to the Bucket before its resources are created. Once the Bucket's S3 resources have been provisioned (by accessing `.resources`), attempting to add new notifications will raise a `RuntimeError`. Define all your notifications immediately after creating the Bucket instance.

#### Available Event Types

| Event Type                                 | Description                     |
|--------------------------------------------|---------------------------------|
| `s3:ObjectCreated:*`                       | All object creation events      |
| `s3:ObjectCreated:Put`                     | Object created via PUT          |
| `s3:ObjectCreated:Post`                    | Object created via POST         |
| `s3:ObjectCreated:Copy`                    | Object created via COPY         |
| `s3:ObjectCreated:CompleteMultipartUpload` | Multipart upload completed      |
| `s3:ObjectRemoved:*`                       | All object removal events       |
| `s3:ObjectRemoved:Delete`                  | Object deleted                  |
| `s3:ObjectRemoved:DeleteMarkerCreated`     | Delete marker created           |
| `s3:ObjectRestore:*`                       | All object restore events       |
| `s3:ObjectTagging:*`                       | All object tagging events       |
| `s3:LifecycleExpiration:*`                 | All lifecycle expiration events |
| `s3:Replication:*`                         | All replication events          |

For a complete list, see the [AWS S3 Event Notifications documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html).

## Parameters

| Parameter    | Description                                                                        |
| ------------ | ---------------------------------------------------------------------------------- |
| `versioning` | The versioning configuration for the S3 bucket. Boolean. Default is `False`.       |
| `access`     | The access configuration for the S3 bucket. Either `None` (default) or `'public'`. |

### Bucket.notify() Parameters

| Parameter       | Description                                                                                                |
| --------------- | ---------------------------------------------------------------------------------------------------------- |
| `name`          | Unique name for this notification subscription (required).                                                 |
| `events`        | List of S3 event types to subscribe to (required).                                                         |
| `filter_prefix` | Filter notifications by object key prefix. Optional.                                                       |
| `filter_suffix` | Filter notifications by object key suffix. Optional.                                                       |
| `function`      | Lambda function handler to invoke. Can be a string, `FunctionConfig`, or `FunctionConfigDict`. Optional.   |
| `queue`         | SQS queue to send notifications to. Can be a `Queue` component or queue ARN string. Optional.              |
| `links`         | List of links to grant the notification function access to other resources (e.g., DynamoDB, queues).       |
| `**opts`        | Additional function configuration options (memory, timeout, etc.) when function is specified as a string.  |

!!! note "Either `function` or `queue` must be specified"
    You must specify exactly one target for the notification - either `function` or `queue`, but not both.

### Resources

| Resource                   | Description                                                                                      |
| -------------------------- | ------------------------------------------------------------------------------------------------ |
| `bucket`                   | The S3 bucket created by the `Bucket` component.                                                 |
| `public_access_block`      | The `BucketPublicAccessBlock` resource created by the `Bucket` component.                        |
| `bucket_policy`            | The `BucketPolicy` resource created by the `Bucket` component if `access` is set to `'public'`.  |
| `bucket_notification`      | The `BucketNotification` resource if any notifications are configured.                           |
| `notification_functions`   | List of Lambda `Function` components created for function notifications.                         |
| `notification_permissions` | List of Lambda `Permission` resources allowing S3 to invoke the notification functions.          |
| `queue_policies`           | List of SQS `QueuePolicy` resources allowing S3 to send messages to notification queues.         |

## Static Websites

Stelvio can create and manage S3 buckets for static website hosting using the `S3StaticWebsite` component.

Create a static website from a directory using the `S3StaticWebsite` component:

```python
@app.run
def run() -> None:
    config = mkdocs.config.load_config("mkdocs.yml")
    mkdocs.commands.build.build(config)

    _ = S3StaticWebsite(
        "s3-static-mkdocs",
        custom_domain="s3-2." + CUSTOM_DOMAIN_NAME,
        directory="site"
    )
```

- Creates an S3 bucket for static website hosting
- Creates a CloudFront distribution for the S3 bucket, so that it is compatible with 3rd party DNS providers
  - Attaching a domain name to a S3 bucket (without CloudFront) only works with AWS Route 53, because you'd need to create a CNAME pointing to the S3 bucket name. This is not possible with other DNS providers as they don't have access to the S3 bucket name in AWS.
- Creates an S3 object for each file in the static website directory
- Automatically creates a DNS record for the CloudFront distribution if a DNS provider is configured

### Handling files (assets) of a static website

The `S3StaticWebsite` component automatically uploads all files in the specified `directory` to the S3 bucket.

The `directory` parameter is optional, though. If omitted, an empty S3 bucket is created and you are responsible for uploading the files (assets) to the bucket.

The `custom_domain` parameter is also optional. If omitted, no DNS record is created for the CloudFront distribution and you can access the static website using the CloudFront domain name (`<distribution_id>.cloudfront.net`).

In the following example, we use the `mkdocs` library to build a static website from Markdown files and upload the generated files to the S3 bucket:

```python
@app.run
def run() -> None:
    config = mkdocs.config.load_config("mkdocs.yml")
    mkdocs.commands.build.build(config)

    website = S3StaticWebsite(
        "s3-static-mkdocs",
        custom_domain="s3-2." + CUSTOM_DOMAIN_NAME,
    )

    # Upload files to the bucket
    s3_bucket = website.bucket
    boto3_client = boto3.client("s3")
    boto3_client.put_object(
        Bucket=s3_bucket.bucket_name, Key="index.html", Body="<h1>Hello, World!</h1>"
    )
```

In both cases, the files uploaded to your static website are considered part of your infrastructure. Thus, the files would be automatically deployed whenever you run `stlv deploy`. However, in the latter case, uploaded files are not part of your (Pulumi) state and thus not tracked. 

Using the `stlv_resources` module, you can access the S3 bucket and manage the files (assets) in your static website, should you decide that part should not be part of your deployment. Note that at the moment, you can get the `arn` of the bucket via `stvl_resources` only from within a Lambda function.

!!! note

    If you decide to upload your file assets manually, you must also take care of removing the files from the bucket before running `stlv destroy`, as the AWS API does not allow deleting a non-empty S3 bucket.

!!! note

    The S3StaticWebsite component is designed for most use cases of static websites. The error handler for 404 is by default set to `error.html`. This will be exposed to the user as a parameter in the future.


### Exposing a bucket along with other resources

If you want to expose a bucket along with other resources, such as an API Gateway, you can use the [`Router` component](/guides/cloudfront-router/).


### Parameters

| Parameter       | Description                                                                                                                                            |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `custom_domain` | The custom domain name for the static website. Optional. If provided, a DNS record will be created for the CloudFront distribution. Optional. A `str`. |
| `directory`     | The directory containing the static website files to be uploaded to the S3 bucket. Optional. Either a `Path` like object or a `str`.                   |

### Resources

| Resource                  | Description                                                 |
| ------------------------- | ----------------------------------------------------------- |
| `bucket`                  | The S3 bucket created for the static website.               |
| `files`                   | The files uploaded to the S3 bucket for the static website. |
| `cloudfront_distribution` | The CloudFront distribution created for the static website. |
